\documentclass[12pt,a4paper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{setspace}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{mathtools}
\usepackage{siunitx}
\usepackage{array}% in the preamble
\usepackage{xcolor}
\usepackage{pagecolor}
\usepackage{lipsum}  
\usepackage{mdframed}
\usepackage{hyperref}


%\pagecolor{black}
%\color{white}



\doublespacing
\title{Resources and Prediction for COVID-19 Pandemic in United States}
\author{Bingan Chen}
\begin{document}
	\maketitle
	\section*{Summary of Research Questions}
	\begin{enumerate}
		\item How to show the ranking about which region are commenting about the medical resources during the pandemic?\\
		- I will display it using the map.
		\item How the increased cases data relates to the commenting?\\
		- The machine learning skills will be critical.
		\item What is the relationship between the commenting percentage and the severity of the pandemic NATIONALLY?\\
		- This may be generalize by the previous question.
		\item How to use the comments to indicate when would the pandemic end?\\
		- Using the ML skills to predict the commenting 'density' by a certain amount of cases.
	\end{enumerate}
	\section*{Motivation}
	While browsing on Twitters during the pandanmic, I noticed that lots of people dead and were sick but weren't treated carefully because of the shortage in medical resources. I want to organize people's comments regarding the medical resources during the pandemic to evaluate how they care about this situation and organize get a possible relationship between the indicated cases and comments. A geographic distribution by state about the comments-density in order to help distribute resources. Also, I want to predict when would the pandemic be not that severe based on the reflection on Twitter.
	\section*{Dataset}
	\begin{itemize}
		\item The data grasped by Twitter API (\url{https://developer.twitter.com/en/docs/twitter-api}) in real-time.
		\item \url{https://covid.cdc.gov/covid-data-tracker/#trends_dailycases}
	\end{itemize}
	\section*{Challenge goals}
	\begin{itemize}
		\item Messy Data: I decided to use the API to get data on Twitter.
		\item New Library: Request may be useful to get the data online.
		\item Multiple Datasets: One grasped by API, the other is from CDC.
	\end{itemize}
	\section*{Method}
	\begin{enumerate}
		\item The geopanda can be used to display the commenting density regarding the pandemic by state, so that the government can distribute the resources.
		\item The machine learning will be applied to learn the relationship to predict the other.
		\item The \texttt{.sum()} to get the national data from states data.
		\item I would determine a level of cases to indicate the 'end' of this pandemic. In this way, there would be a method to reflect the severity by the comments.
	\end{enumerate}
	
	\section*{Work Plan}
	\begin{enumerate}
		\item Learn to use the Twitter API to filter the data by content. (20 hrs)
		\item Write snippets to test the functions and filter. (15 hrs)
		\item Learn to change the returned \texttt{.json} to usable \texttt{.csv}file. (10 hrs)
		\item Determine which part of data is for test that wouldn't be trained. (5 hrs)
		\item Integrate the functions and organize data. (20 hrs)
		\item Finalize and test the codes. (15 hrs)
	\end{enumerate}
	\subsection*{Dev Environment}
	Locally on my laptop.
\end{document}